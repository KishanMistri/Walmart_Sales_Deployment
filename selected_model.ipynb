{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b26b20b-ee95-4d7b-bff6-d745301990cc",
   "metadata": {},
   "source": [
    "<h2><center>Walmart Sales Forecasting</center></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e312353-3384-4b75-bed2-6e463ce96f1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T15:43:41.067339Z",
     "iopub.status.busy": "2022-10-15T15:43:41.067070Z",
     "iopub.status.idle": "2022-10-15T15:43:42.365486Z",
     "shell.execute_reply": "2022-10-15T15:43:42.364923Z",
     "shell.execute_reply.started": "2022-10-15T15:43:41.067299Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from downcast import reduce\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9697a119-f406-4a41-96fd-d6d833a09e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T15:43:42.367107Z",
     "iopub.status.busy": "2022-10-15T15:43:42.366725Z",
     "iopub.status.idle": "2022-10-15T15:43:42.371367Z",
     "shell.execute_reply": "2022-10-15T15:43:42.370702Z",
     "shell.execute_reply.started": "2022-10-15T15:43:42.367090Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Env variables\n",
    "non_day_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "level_groupings = {1: [],   #Day sales accorss everything. [Results in single row with column of each day sales.]\n",
    "                   2: [\"state_id\"], \n",
    "                   3: [\"store_id\"], \n",
    "                   4: [\"cat_id\"], \n",
    "                   5: [\"dept_id\"], \n",
    "                   6: [\"state_id\", \"cat_id\"], \n",
    "                   7: [\"state_id\", \"dept_id\"], \n",
    "                   8: [\"store_id\", \"cat_id\"], \n",
    "                   9: [\"store_id\", \"dept_id\"],\n",
    "                   10:[\"item_id\"], \n",
    "                   11:[\"item_id\", \"state_id\"],\n",
    "                   12:[]  #[\"item_id\", \"store_id\"] == 'id'\n",
    "                   }\n",
    "project_path = \"~/Desktop/workspace/Walmart_Sales_Deployment/\"\n",
    "data_dir = project_path+\"dataset/\"\n",
    "processed_fpath = project_path+'processing_data/'\n",
    "submission_dir = project_path+'outputs/'\n",
    "\n",
    "random_state = 11223300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44f4bb-1168-4a49-8202-1987842ade8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T15:43:42.372947Z",
     "iopub.status.busy": "2022-10-15T15:43:42.372530Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sales=pd.read_csv(data_dir+'sales_train_evaluation.csv')\n",
    "calendar=pd.read_csv(data_dir+'calendar.csv')\n",
    "prices=pd.read_csv(data_dir+'sell_prices.csv')\n",
    "\n",
    "# This will take some time\n",
    "sales=reduce(sales)\n",
    "calendar=reduce(calendar)\n",
    "prices=reduce(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8609199f-3a85-4985-8f1c-49abc58a9460",
   "metadata": {},
   "source": [
    "### Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d23da8d-28dd-4338-b91f-46452f72c495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If day is weekend or not\n",
    "calendar['Weekend'] = calendar['weekday'].apply(lambda day: 1 if day.lower() == 'saturday' or day.lower() == 'sunday' else 0).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba31107-8c59-457c-b472-a3d80960484f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get day of the Month from data object\n",
    "calendar['Day_Of_Month'] = calendar['date'].dt.day.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b190d39-7d4d-4774-8353-52a0513f7c88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OnehotEncoding for event types\n",
    "OHE_df = pd.get_dummies(calendar[['event_type_1','event_type_1']], prefix=['ET1', 'ET2'],dtype = np.int8)\n",
    "cols = OHE_df.columns\n",
    "OHE_df['total_events'] = OHE_df[cols].sum(axis = 1).astype(np.int8)\n",
    "calendar = pd.concat([calendar,OHE_df],axis=1)\n",
    "del OHE_df\n",
    "del cols\n",
    "calendar.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e1ad5-e5a3-4461-bf36-984e3b6e1bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop reduandant columns\n",
    "calendar.drop(['date','weekday','wday','event_name_1','event_type_1','event_name_2','event_type_2'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef576a-00d2-4c06-b71e-e62d5cde4c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "calendar = reduce(calendar)\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb3e2b-bef9-4390-acdf-031ce63fa006",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sales[['id','item_id','dept_id','cat_id','store_id','state_id']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb1bbec-5be5-4d85-8821-8897b7177bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "sales=pd.melt(sales,id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'],var_name='d',value_name='units_sold')\n",
    "sales=pd.merge(sales,calendar,on='d',how='left')\n",
    "sales=pd.merge(sales,prices,on=['item_id','store_id','wm_yr_wk'],how='left')\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf0247-68f8-4013-a4cf-b3e23f348a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# No need after melt & merging\n",
    "del calendar\n",
    "del prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387c2ad5-c708-4d30-a753-217ae18d2b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To impute the NaN's of sell_prices, we will take average prices of product grouped.\n",
    "sales['sell_price']=sales['sell_price'].fillna(sales.groupby('id')['sell_price'].transform('mean'))\n",
    "sales.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e44e3-3dd9-4f6d-a02c-9b14d591d7cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Store the categories along with their codes\n",
    "map_id = dict(zip(sales.id.cat.codes, sales.id))\n",
    "map_item_id = dict(zip(sales.item_id.cat.codes, sales.item_id))\n",
    "map_dept_id = dict(zip(sales.dept_id.cat.codes, sales.dept_id))\n",
    "map_cat_id = dict(zip(sales.cat_id.cat.codes, sales.cat_id))\n",
    "map_store_id = dict(zip(sales.store_id.cat.codes, sales.store_id))\n",
    "map_state_id = dict(zip(sales.state_id.cat.codes, sales.state_id))\n",
    "\n",
    "joblib.dump(map_id, processed_fpath+'map_id.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e782f-967c-4d02-845d-f12953cb5201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Labelling the categories provided.\n",
    "category=['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "for cat in tqdm(category):\n",
    "    sales[cat] = sales[cat].cat.codes\n",
    "sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8934214-f216-485c-a252-37cf27ebbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['d']  = sales['d'].apply(lambda s: int(s.split('_')[1]))\n",
    "sales = reduce(sales)\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83514d1a-875f-46a9-b444-4f5078e08f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_df = sales.groupby(['id'])['units_sold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e98ba38-8be2-4d75-8516-ff633e01e936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rolling Window Statistics\n",
    "window_size = [7,14,21,28,35,42,49,56,63] \n",
    "for window in tqdm(window_size):\n",
    "    sales['mean_units_rolling_'+str(window)] = intermediate_df\\\n",
    "                                .transform(lambda x: x.rolling(window=window).mean())\\\n",
    "                                .fillna(0)\\\n",
    "                                .astype(np.float16)\n",
    "    \n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06b0271-a3ee-4433-b2c0-928349f3eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lags\n",
    "most_useful_lags = [7,14,21,28,35,42,70]\n",
    "for lag in tqdm(most_useful_lags):\n",
    "    sales['lag_'+str(lag)] = intermediate_df.shift(lag).fillna(0)\n",
    "    \n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863882d2-01b3-47ab-bcdd-874b118d6f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window median and Standard Deviation\n",
    "window_size = [7,14,21,28,35,42,70] \n",
    "for window in window_size:\n",
    "    sales['median_units_rolling_'+str(window)] = intermediate_df\\\n",
    "                                    .transform(lambda x: x.rolling(window=window).median())\\\n",
    "                                    .astype(np.float16)\n",
    "    sales['std_units_rolling_'+str(window)] = intermediate_df\\\n",
    "                                    .transform(lambda x: x.rolling(window=window).std())\\\n",
    "                                    .astype(np.float16)\n",
    "\n",
    "sales.fillna(0, inplace=True)\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3ab2e3-b13d-4eb8-9ac6-d371af27a642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanding Window Stats\n",
    "for window in window_size:\n",
    "    sales['mean_units_expanding_'+str(window)] = intermediate_df\\\n",
    "                                      .transform(lambda x: x.expanding(window).mean())\\\n",
    "                                      .astype(np.float16)\n",
    "\n",
    "    sales['median_units_expanding_'+str(window)] = intermediate_df\\\n",
    "                                      .transform(lambda x: x.expanding(window).median())\\\n",
    "                                      .astype(np.float16)\n",
    "\n",
    "    sales['std_units_expanding_'+str(window)] = intermediate_df\\\n",
    "                                      .transform(lambda x: x.expanding(window).median())\\\n",
    "                                      .astype(np.float16)\n",
    "\n",
    "sales.fillna(0, inplace=True)    \n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19b9c24-0659-4f38-ae58-f28c85fa0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selling Trend feature\n",
    "sales['daily_avg_units'] = sales.groupby(['id','d'])['units_sold'].transform('mean').fillna(0)\n",
    "sales['avg_units'] = intermediate_df.transform('mean').fillna(0)\n",
    "\n",
    "sales['selling_trend'] = (sales['daily_avg_units'] - sales['avg_units'])\n",
    "\n",
    "sales.drop(['daily_avg_units','avg_units'],axis=1,inplace=True)\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0879fd-cf7e-4eab-b45b-5508e6507554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Encoding features\n",
    "sales['mean_units_by_item'] = df.groupby('item_id',as_index=False, sort=False)['units']\\\n",
    "                                                     .transform('mean')\\\n",
    "                                                     .astype(np.float16)\n",
    "sales['mean_units_by_state'] = df.groupby('state_id',as_index=False, sort=False)['units']\\\n",
    "                                                      .transform('mean')\\\n",
    "                                                      .astype(np.float16)\n",
    "sales['mean_units_by_store'] = df.groupby('store_id',as_index=False, sort=False)['units']\\\n",
    "                                                      .transform('mean')\\\n",
    "                                                      .astype(np.float16)\n",
    "sales['mean_units_by_cat'] = df.groupby('cat_id',as_index=False, sort=False)['units']\\\n",
    "                                                    .transform('mean')\\\n",
    "                                                    .astype(np.float16)\n",
    "sales['mean_units_by_dept'] = df.groupby('dept_id',as_index=False, sort=False)['units']\\\n",
    "                                                     .transform('mean')\\\n",
    "                                                     .astype(np.float16)\n",
    "\n",
    "sales['mean_units_by_cat_dept'] = df.groupby(['cat_id','dept_id'],as_index=False, sort=False)['units']\\\n",
    "                                                         .transform('mean')\\\n",
    "                                                         .astype(np.float16)\n",
    "sales['mean_units_by_store_item'] = df.groupby(['store_id','item_id'],as_index=False, sort=False)['units']\\\n",
    "                                                           .transform('mean')\\\n",
    "                                                           .astype(np.float16)\n",
    "sales['mean_units_by_cat_item'] = df.groupby(['cat_id','item_id'],as_index=False, sort=False)['units']\\\n",
    "                                                         .transform('mean')\\\n",
    "                                                         .astype(np.float16)\n",
    "sales['mean_units_by_dept_item'] = df.groupby(['dept_id','item_id'],as_index=False, sort=False)['units']\\\n",
    "                                                          .transform('mean')\\\n",
    "                                                          .astype(np.float16)\n",
    "sales['mean_units_by_state_store'] = df.groupby(['state_id','store_id'],as_index=False, sort=False)['units']\\\n",
    "                                                            .transform('mean')\\\n",
    "                                                            .astype(np.float16)\n",
    "\n",
    "sales['mean_units_by_state_store_cat'] = df.groupby(['state_id','store_id','cat_id'],as_index=False, sort=False)['units']\\\n",
    "                                                                .transform('mean')\\\n",
    "                                                                .astype(np.float16)\n",
    "sales['mean_units_by_store_cat_dept'] = df.groupby(['store_id','cat_id','dept_id'],as_index=False, sort=False)['units']\\\n",
    "                                                               .transform('mean')\\\n",
    "                                                               .astype(np.float16)\n",
    "\n",
    "sales.fillna(0, inplace=True)\n",
    "sales.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df1a95-c919-4328-a4f5-5da55363bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Metric function - RMSSE\n",
    "\n",
    "def RMSE(actual,predictions):\n",
    "    if type(actual) != 'numpy.ndarray':\n",
    "        actual = np.array(actual)\n",
    "    if type(predictions) != 'numpy.ndarray':\n",
    "        predictions = np.array(predictions)\n",
    "    assert actual.shape[0] == predictions.shape[0], \"Observation count Mismatched...!\"\n",
    "    assert len(actual.shape) == 1 and len(predictions.shape) == 1, \"RMSE takes single dimension list...\"\n",
    "    \n",
    "    return round(np.sqrt(((actual-predictions)**2).mean()),3)\n",
    "\n",
    "def RMSSE(y_actual, y_pred, train_series, axis=1):\n",
    "    assert axis == 0 or axis == 1\n",
    "    if type(y_actual) != 'numpy.ndarray': y_actual = np.array(y_actual)\n",
    "    if type(y_pred) != 'numpy.ndarray': y_pred = np.array(y_pred)\n",
    "    if type(train_series) != 'numpy.ndarray': train_series = np.array(train_series)\n",
    "    \n",
    "    assert y_actual.shape == y_pred.shape\n",
    "    \n",
    "    if axis == 1:\n",
    "        # using axis == 1 we must guarantee these are matrices and not arrays\n",
    "        assert y_actual.shape[1] > 1 and y_pred.shape[1] > 1 and train_series.shape[1] > 1\n",
    "    \n",
    "    numerator = ((y_actual - y_pred)**2).sum(axis=axis)\n",
    "    n = train_cols\n",
    "    \n",
    "    if axis == 1:\n",
    "        denominator = 1/(n-1) * ((train_series[:, 1:] - train_series[:, :-1]) ** 2).sum(axis=axis)\n",
    "    else:\n",
    "        denominator = 1/(n-1) * ((train_series[1:] - train_series[:-1]) ** 2).sum(axis=axis)\n",
    "    return ((1/forcasting_horizon) * numerator/denominator) ** 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90ca347-6bba-4113-88d9-4bc274d3e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility Functions\n",
    "# Submission Format df\n",
    "def submission(X_test, y_test, predictions):\n",
    "    \"\"\"\n",
    "    >> submission(X_test, y_test, predictions)\n",
    "    Inputs:\n",
    "        i)   X_test must have 2 columns: \n",
    "                -> 'id' : integer values id's which will be used for getting mapped product ids\n",
    "                -> 'd'  : Day num columns with integer values only\n",
    "        ii)  y_test is original answers for ['id','d'] combinations\n",
    "        iii) predictions are pred values\n",
    "    \n",
    "    Returns:\n",
    "        df in final submission format\n",
    "    \"\"\"\n",
    "    # Getting just important cols\n",
    "    df = X_test[['id','d']]\n",
    "    df['actual'] = y_test\n",
    "    df['prediction'] = predictions\n",
    "    \n",
    "    # F_ day formatted columns\n",
    "    start = df.d.min() - 1\n",
    "    df['d'] = df['d'] - start\n",
    "    \n",
    "    # Pivot and Id string mappings\n",
    "    id_map = joblib.load('./pre-processed/encoded/map_id.pkl')\n",
    "    \n",
    "    valid_df = pd.pivot(df,index='id',values ='prediction',columns = 'd').reset_index()\n",
    "    valid_df['id'] = valid_df['id'].map(id_map).str.replace('evaluation','validation')\n",
    "    \n",
    "    eval_df = pd.pivot(df,index='id',values ='actual',columns = 'd').reset_index()\n",
    "    eval_df['id'] = eval_df['id'].map(id_map)\n",
    "    \n",
    "    results = pd.concat([valid_df,eval_df])\n",
    "    results = results.set_index('id').add_prefix('F').reset_index()\n",
    "    del valid_df\n",
    "    del eval_df\n",
    "    \n",
    "    return results\n",
    "    \n",
    "\n",
    "# To get RandomizedSearchCV with TimeSplit indexes provided\n",
    "def get_RS_model(estimator=None, param_distributions=None ,n_iter=3,n_jobs=1):\n",
    "    \"\"\"\n",
    "    Get the RandomizedSearchCV model object with 3-Fold TimeSplit of X_train data with Default 3 Randomized sampled parameters from each HyperParameter\n",
    "    Inputs: 1) estimator 2) param_distributions for given estimator\n",
    "    Output: RandomizedSearch \n",
    "    \"\"\"\n",
    "    return  RandomizedSearchCV(estimator, param_distributions, cv=train_test_idx,\\\n",
    "                               n_iter=n_iter, scoring='neg_mean_squared_error', n_jobs=1,random_state=random_state,\\\n",
    "                               refit=False, verbose=2, return_train_score=True)\n",
    "\n",
    "# To get feature importance bar Chart\n",
    "def plot_feature_importance(model = None, test_df = None, skip = 0):\n",
    "    \"\"\"\n",
    "    >>> Usage: plot_feature_importance(model = estimator, test_df = X_test, skip = 0)\n",
    "    skip parameter is used when the number of features are large and you want leave out/skip the least k important points in graph.\n",
    "    \"\"\"\n",
    "    features = test_df.columns\n",
    "    if hasattr(model,'coef_'):\n",
    "        importances = model.coef_\n",
    "    \n",
    "    elif hasattr(model,'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "    if skip != 0:\n",
    "        indices = np.argsort(importances)[skip:]\n",
    "    else:\n",
    "        indices = np.argsort(importances)\n",
    "        \n",
    "    plt.figure(figsize=(6,8))\n",
    "    plt.barh(range(len(indices)) , importances[indices])\n",
    "    \n",
    "    plt.title('Feature Importance',fontsize=14)\n",
    "    plt.xlabel('Relative Importance',fontsize=14)\n",
    "    plt.yticks(range(len(indices)),[features[i] for i in indices])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381fb10-daf1-4b15-8e0b-d9f973b9d4fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joblib.dump(sales,processed_fpath+'featured_data.pkl')\n",
    "del sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ca59ac-a8dc-4421-82c9-ab97b5f99eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing featured data from Phase-3.1 \n",
    "data = joblib.load(processed_fpath+'featured_data.pkl')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9f3f7-cafa-467c-bbde-e497d9575b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Val-Test set split\n",
    "X_train = data.loc[data.d < 1886]\n",
    "y_train = X_train['units_sold']\n",
    "X_train.drop(['units_sold'], axis=1, inplace=True)\n",
    "\n",
    "# Validation dataset\n",
    "validation_days=np.arange(1886,1914)\n",
    "X_val = data.loc[data.d.isin(validation_days)]\n",
    "y_val = X_val['units_sold']\n",
    "X_val.drop(['units_sold'], axis=1, inplace=True)\n",
    "\n",
    "# Test set\n",
    "X_test  = data.loc[data.d > 1913]\n",
    "y_test  = X_test['units_sold']\n",
    "X_test.drop(['units_sold'], axis=1, inplace=True)\n",
    "\n",
    "print(\"X_train: {0} \\t y_train: {1}\\n\".format(X_train.shape,y_train.shape))\n",
    "print(\"X_val:\\t {0}\\t\\t y_val:\\t {1}\\n\".format(X_val.shape,y_val.shape))\n",
    "print(\"X_test:\\t {0}\\t\\t y_test: {1}\\n\".format(X_test.shape,y_test.shape))\n",
    "\n",
    "# For space issues\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b41b5-0cb5-4bbd-9f28-7cb2b086c720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Getting best model fit for predictions\n",
    "model = LGBMRegressor(learning_rate=0.035,\\\n",
    "                      max_depth=148,\\\n",
    "                      num_leaves=375,\\\n",
    "                      lambda_l2=0.05,\\\n",
    "                      n_estimators=100, random_state = random_state)\n",
    "\n",
    "for k in range(len(train_test_idx)):\n",
    "    train_idx, val_idx = train_test_idx[k]\n",
    "    X = X_train.iloc[train_idx]\n",
    "    y = y_train.iloc[train_idx]\n",
    "    model.fit(X,y)\n",
    "    cv_hat = model.predict(X_train.iloc[val_idx])\n",
    "    print(\"CV-{0} Error: {1}\".format(k+1,RMSE(y_train.iloc[val_idx],cv_hat)))\n",
    "\n",
    "# Validation score\n",
    "Y_hat = model.predict(X_val)\n",
    "print(\"\\nValidation Error: \",RMSE(y_val,Y_hat))\n",
    "\n",
    "model.fit(X_val,y_val)\n",
    "# Test scores\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"\\nTest Error: \",RMSE(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe40a8b-6188-4604-af8b-e96d4f394d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Feature Importances\n",
    "plot_feature_importance(model = model, test_df = X_test,skip=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f2cc21-1f53-48ca-abe7-af54ed6e2510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Submission Format df\n",
    "sub_df = submission(X_test, y_test, predictions = y_pred)\n",
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42b6ca-59a0-4cf5-940f-844a93a4f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving df and Model\n",
    "sub_df.to_csv(submission_dir+'submission_LightGBM.csv',index=False)\n",
    "joblib.dump(model,models_dir+'Model_LightGBM.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec82ee-32d4-41b3-be7a-f0ab5e2d7ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup \n",
    "cleanup_list = [sub_df, Y_hat, y_pred, model, X, y, cv_hat]\n",
    "for obj in cleanup_list:\n",
    "    del obj\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0a0ae5-1917-417c-9b67-29e6205ef57b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
